{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== WEEK 5: DATA PROFILE ===\n",
      "Shape: (497, 21)\n",
      "Missing per column:\n",
      " date                     0\n",
      "Drink (Coffee)           0\n",
      "Drink (Non Coffee)       0\n",
      "Food                     0\n",
      "Other                    0\n",
      "Retail                   0\n",
      "Total Sales              0\n",
      "Academic                 0\n",
      "Campus                   0\n",
      "Community                0\n",
      "Holiday                  0\n",
      "Popular                  0\n",
      "Sports                   0\n",
      "max_temp_celsius        33\n",
      "min_temp_celsius        33\n",
      "precipitation_mm        33\n",
      "max_wind_speed_kmh      33\n",
      "max_temp_fahrenheit     33\n",
      "min_temp_fahrenheit     33\n",
      "max_wind_speed_mph      33\n",
      "precipitation_inches    33\n",
      "dtype: int64\n",
      "Duplicate dates?  False\n",
      "\n",
      "Descriptive stats:\n",
      "                       count unique        top freq      first       last  \\\n",
      "date                    497    497 2024-01-01    1 2024-01-01 2025-05-11   \n",
      "Drink (Coffee)        497.0    NaN        NaT  NaN        NaT        NaT   \n",
      "Drink (Non Coffee)    497.0    NaN        NaT  NaN        NaT        NaT   \n",
      "Food                  497.0    NaN        NaT  NaN        NaT        NaT   \n",
      "Other                 497.0    NaN        NaT  NaN        NaT        NaT   \n",
      "Retail                497.0    NaN        NaT  NaN        NaT        NaT   \n",
      "Total Sales           497.0    NaN        NaT  NaN        NaT        NaT   \n",
      "Academic              497.0    NaN        NaT  NaN        NaT        NaT   \n",
      "Campus                497.0    NaN        NaT  NaN        NaT        NaT   \n",
      "Community             497.0    NaN        NaT  NaN        NaT        NaT   \n",
      "Holiday               497.0    NaN        NaT  NaN        NaT        NaT   \n",
      "Popular               497.0    NaN        NaT  NaN        NaT        NaT   \n",
      "Sports                497.0    NaN        NaT  NaN        NaT        NaT   \n",
      "max_temp_celsius      464.0    NaN        NaT  NaN        NaT        NaT   \n",
      "min_temp_celsius      464.0    NaN        NaT  NaN        NaT        NaT   \n",
      "precipitation_mm      464.0    NaN        NaT  NaN        NaT        NaT   \n",
      "max_wind_speed_kmh    464.0    NaN        NaT  NaN        NaT        NaT   \n",
      "max_temp_fahrenheit   464.0    NaN        NaT  NaN        NaT        NaT   \n",
      "min_temp_fahrenheit   464.0    NaN        NaT  NaN        NaT        NaT   \n",
      "max_wind_speed_mph    464.0    NaN        NaT  NaN        NaT        NaT   \n",
      "precipitation_inches  464.0    NaN        NaT  NaN        NaT        NaT   \n",
      "\n",
      "                             mean         std       min       25%       50%  \\\n",
      "date                          NaN         NaN       NaN       NaN       NaN   \n",
      "Drink (Coffee)         747.288109  467.729717       0.0    433.46    802.17   \n",
      "Drink (Non Coffee)     456.493058  349.317372       0.0    163.97    430.92   \n",
      "Food                    242.14336  163.293959       0.0    121.03    251.08   \n",
      "Other                   41.857907  115.354613       0.0       0.0       0.0   \n",
      "Retail                  16.001026   27.651804       0.0       0.0       0.0   \n",
      "Total Sales           1503.783461  962.857318       0.0    775.64   1683.06   \n",
      "Academic                 0.046278    0.210297       0.0       0.0       0.0   \n",
      "Campus                   0.020121    0.140555       0.0       0.0       0.0   \n",
      "Community                0.040241    0.196723       0.0       0.0       0.0   \n",
      "Holiday                  0.024145    0.153654       0.0       0.0       0.0   \n",
      "Popular                  0.066398    0.249228       0.0       0.0       0.0   \n",
      "Sports                   0.046278    0.210297       0.0       0.0       0.0   \n",
      "max_temp_celsius        15.485991   11.171736     -15.9     6.675      16.5   \n",
      "min_temp_celsius          5.88125   10.356564     -23.3      -1.5       5.7   \n",
      "precipitation_mm         2.592888    6.108898       0.0       0.0       0.0   \n",
      "max_wind_speed_kmh      21.306034    6.952592       6.8      16.2      20.7   \n",
      "max_temp_fahrenheit     59.874784   20.109125      3.38    44.015      61.7   \n",
      "min_temp_fahrenheit      42.58625   18.641815     -9.94      29.3     42.26   \n",
      "max_wind_speed_mph      13.238952    4.320139  4.225323  10.06621  12.86238   \n",
      "precipitation_inches     0.102082    0.240508       0.0       0.0       0.0   \n",
      "\n",
      "                            75%        max  \n",
      "date                        NaN        NaN  \n",
      "Drink (Coffee)          1039.86    2584.82  \n",
      "Drink (Non Coffee)        712.9    1904.93  \n",
      "Food                     354.82     827.88  \n",
      "Other                     19.65     1075.0  \n",
      "Retail                     21.0      259.6  \n",
      "Total Sales              2245.0    4576.73  \n",
      "Academic                    0.0        1.0  \n",
      "Campus                      0.0        1.0  \n",
      "Community                   0.0        1.0  \n",
      "Holiday                     0.0        1.0  \n",
      "Popular                     0.0        1.0  \n",
      "Sports                      0.0        1.0  \n",
      "max_temp_celsius           24.9       34.5  \n",
      "min_temp_celsius         14.725       24.2  \n",
      "precipitation_mm          1.825       59.1  \n",
      "max_wind_speed_kmh         25.8       46.3  \n",
      "max_temp_fahrenheit       76.82       94.1  \n",
      "min_temp_fahrenheit      58.505      75.56  \n",
      "max_wind_speed_mph    16.031372  28.769477  \n",
      "precipitation_inches    0.07185   2.326772  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_h/yxt5btd56fdfcnmx2q8hfr3m0000gn/T/ipykernel_8504/1545815962.py:112: FutureWarning: Treating datetime data as categorical rather than numeric in `.describe` is deprecated and will be removed in a future version of pandas. Specify `datetime_is_numeric=True` to silence this warning and adopt the future behavior now.\n",
      "  print(\"\\nDescriptive stats:\\n\", df.describe(include=\"all\").T)\n",
      "/var/folders/_h/yxt5btd56fdfcnmx2q8hfr3m0000gn/T/ipykernel_8504/1545815962.py:141: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n",
      "  plt.tight_layout()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== WEEK 6: MISSING-VALUE HANDLING ===\n",
      "Missing before:\n",
      " Drink (Coffee)           0\n",
      "Drink (Non Coffee)       0\n",
      "Food                     0\n",
      "Other                    0\n",
      "Retail                   0\n",
      "Total Sales              0\n",
      "Academic                 0\n",
      "Campus                   0\n",
      "Community                0\n",
      "Holiday                  0\n",
      "Popular                  0\n",
      "Sports                   0\n",
      "max_temp_celsius        33\n",
      "min_temp_celsius        33\n",
      "precipitation_mm        33\n",
      "max_wind_speed_kmh      33\n",
      "max_temp_fahrenheit     33\n",
      "min_temp_fahrenheit     33\n",
      "max_wind_speed_mph      33\n",
      "precipitation_inches    33\n",
      "dtype: int64\n",
      "Missing after:\n",
      " Drink (Coffee)          0\n",
      "Drink (Non Coffee)      0\n",
      "Food                    0\n",
      "Other                   0\n",
      "Retail                  0\n",
      "Total Sales             0\n",
      "Academic                0\n",
      "Campus                  0\n",
      "Community               0\n",
      "Holiday                 0\n",
      "Popular                 0\n",
      "Sports                  0\n",
      "max_temp_celsius        0\n",
      "min_temp_celsius        0\n",
      "precipitation_mm        0\n",
      "max_wind_speed_kmh      0\n",
      "max_temp_fahrenheit     0\n",
      "min_temp_fahrenheit     0\n",
      "max_wind_speed_mph      0\n",
      "precipitation_inches    0\n",
      "dtype: int64\n",
      "[Week 7] Flagged 1 IQR outliers and 1 Z-score outliers\n",
      "[Week 8] Found 0 rows in duplicate-date groups\n",
      "[Week 8] Dropped 0 duplicates; 497 rows remain\n",
      "\n",
      "=== WEEK 10: SYNTACTIC CLEANING ===\n",
      " • No product_category column to clean\n",
      "\n",
      "=== WEEK 11: CONSTRAINT VALIDATION ===\n",
      " • Found 0 negative Total Sales entries\n",
      " • Found 497 rows where category sums ≠ Total Sales\n",
      " • Marked 14 federal holidays\n",
      "\n",
      "=== WEEK 12: PROBABILISTIC SIGNALS ===\n",
      " • Computed probabilistic error signal for 15 rows\n",
      "\n",
      "=== WEEK 13: INTEGRATION CHECKS ===\n",
      " • Missing weather on 0 days\n",
      "\n",
      "=== WEEK 14: PROVENANCE & ENVIRONMENT ===\n",
      "✓ Wrote environment snapshot to /Users/albertchen/Downloads/IS 537/env_snapshot.json\n",
      "\n",
      "=== WEEK 15: REPRODUCIBILITY ===\n",
      " • Random seeds set; code fully versioned in GitHub repo;\n",
      "   env snapshot & profiling artifacts saved for full traceability\n",
      "\n",
      "✅ All done — combined data → /Users/albertchen/Downloads/IS 537/combined_data_full.csv\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import platform\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats  # for z-score\n",
    "from pandas.tseries.holiday import USFederalHolidayCalendar\n",
    "import scipy\n",
    "# ----------------------------------------\n",
    "# 1) Load the external category mapping\n",
    "def load_category_mapping(path=\"categories.json\"):\n",
    "    with open(path, \"r\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# ----------------------------------------\n",
    "# 2) Read & parse the raw category-sales CSV\n",
    "def read_sales_data(file_path):\n",
    "    header_row = None\n",
    "    with open(file_path, \"r\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if re.match(r\"^\\s*Category\", line):\n",
    "                header_row = i\n",
    "                break\n",
    "    if header_row is None:\n",
    "        raise ValueError(f\"No 'Category' header found in {file_path}\")\n",
    "\n",
    "    df = pd.read_csv(\n",
    "        file_path,\n",
    "        header=header_row,\n",
    "        engine=\"python\",\n",
    "        skipfooter=1\n",
    "    )\n",
    "    df.set_index(df.columns[0], inplace=True)\n",
    "    df.drop(\"Total\", errors=\"ignore\", inplace=True)\n",
    "\n",
    "    # strip $ and commas\n",
    "    for col in df.columns:\n",
    "        df[col] = (\n",
    "            df[col]\n",
    "              .astype(str)\n",
    "              .str.replace(r\"[\\$,]\", \"\", regex=True)\n",
    "              .astype(float)\n",
    "        )\n",
    "    return df\n",
    "\n",
    "# ----------------------------------------\n",
    "# 3) Aggregate detailed → broad categories\n",
    "def aggregate_sales_by_category(sales_df, category_mapping):\n",
    "    df_long = (\n",
    "        sales_df\n",
    "          .reset_index()\n",
    "          .melt(id_vars=\"Category\", var_name=\"date\", value_name=\"sales\")\n",
    "          .rename(columns={\"Category\":\"category\"})\n",
    "    )\n",
    "    df_long[\"date\"] = pd.to_datetime(df_long[\"date\"])\n",
    "    df_long[\"broad\"] = df_long[\"category\"].map(category_mapping).fillna(\"Other\")\n",
    "\n",
    "    df_pivot = (\n",
    "        df_long\n",
    "          .pivot_table(\n",
    "            index=\"date\",\n",
    "            columns=\"broad\",\n",
    "            values=\"sales\",\n",
    "            aggfunc=\"sum\"\n",
    "          )\n",
    "          .fillna(0)\n",
    "    )\n",
    "    df_pivot[\"Total Sales\"] = df_pivot.sum(axis=1)\n",
    "    return df_pivot\n",
    "\n",
    "# ----------------------------------------\n",
    "# 4) Parse events into one-hot pivots\n",
    "def parse_events(file_path):\n",
    "    text = Path(file_path).read_text()\n",
    "    matches = re.findall(r\"\\*\\*(\\d+/\\d+/\\d+)\\*\\* - .*?\\[(.*?)\\]\", text)\n",
    "    if not matches:\n",
    "        return pd.DataFrame()\n",
    "    rows = [\n",
    "        {\n",
    "          \"date\": pd.to_datetime(date_str, format=\"%m/%d/%Y\"),\n",
    "          \"event\": category\n",
    "        }\n",
    "        for date_str, category in matches\n",
    "    ]\n",
    "    df = (\n",
    "        pd.DataFrame(rows)\n",
    "          .assign(present=1)\n",
    "          .pivot_table(\n",
    "             index=\"date\",\n",
    "             columns=\"event\",\n",
    "             values=\"present\",\n",
    "             fill_value=0\n",
    "          )\n",
    "          .rename_axis(columns=None)\n",
    "    )\n",
    "    return df\n",
    "\n",
    "# ----------------------------------------\n",
    "# WEEK 5: Exploratory Data Analysis & Profiling\n",
    "def profile_data(df, output_dir: Path):\n",
    "    output_dir.mkdir(exist_ok=True, parents=True)\n",
    "    print(\"\\n=== WEEK 5: DATA PROFILE ===\")\n",
    "    print(\"Shape:\", df.shape)\n",
    "    print(\"Missing per column:\\n\", df.isna().sum())\n",
    "    print(\"Duplicate dates? \", df.duplicated(subset=[\"date\"]).any())\n",
    "    print(\"\\nDescriptive stats:\\n\", df.describe(include=\"all\").T)\n",
    "\n",
    "    # Time series plot of Total Sales\n",
    "    if \"Total Sales\" in df.columns:\n",
    "        plt.figure(figsize=(10,4))\n",
    "        df.set_index(\"date\")[\"Total Sales\"].plot(title=\"Total Sales over Time\")\n",
    "        plt.ylabel(\"Sales ($)\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(output_dir / \"ts_total_sales.png\")\n",
    "        plt.close()\n",
    "\n",
    "        # Histogram\n",
    "        plt.figure(figsize=(6,4))\n",
    "        df[\"Total Sales\"].hist(bins=30)\n",
    "        plt.title(\"Distribution of Total Sales\")\n",
    "        plt.xlabel(\"Sales ($)\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(output_dir / \"hist_total_sales.png\")\n",
    "        plt.close()\n",
    "\n",
    "    # Correlation heatmap\n",
    "    num = df.select_dtypes(include=\"number\")\n",
    "    corr = num.corr()\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.matshow(corr, fignum=1)\n",
    "    plt.title(\"Correlation Matrix\", pad=20)\n",
    "    plt.xticks(range(len(corr)), corr.columns, rotation=90)\n",
    "    plt.yticks(range(len(corr)), corr.columns)\n",
    "    plt.colorbar()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_dir / \"corr_matrix.png\")\n",
    "    plt.close()\n",
    "\n",
    "    # Save summary table\n",
    "    summary = pd.DataFrame({\n",
    "        \"dtype\": df.dtypes,\n",
    "        \"missing\": df.isna().sum(),\n",
    "        \"unique\": df.nunique(),\n",
    "        \"mean\": df.mean(numeric_only=True),\n",
    "        \"std\": df.std(numeric_only=True),\n",
    "        \"min\": df.min(numeric_only=True),\n",
    "        \"max\": df.max(numeric_only=True),\n",
    "    })\n",
    "    summary.to_csv(output_dir / \"data_profile_summary.csv\")\n",
    "\n",
    "# ----------------------------------------\n",
    "# WEEK 6: Missing-Value Handling\n",
    "def handle_missing_values(df, output_dir: Path):\n",
    "    output_dir.mkdir(exist_ok=True, parents=True)\n",
    "    print(\"\\n=== WEEK 6: MISSING-VALUE HANDLING ===\")\n",
    "    before = df.isna().sum()\n",
    "    print(\"Missing before:\\n\", before)\n",
    "\n",
    "    # Impute numeric columns with median\n",
    "    medians = df.median(numeric_only=True)\n",
    "    df_imputed = df.fillna(medians)\n",
    "\n",
    "    after = df_imputed.isna().sum()\n",
    "    print(\"Missing after:\\n\", after)\n",
    "\n",
    "    # Log report\n",
    "    report = pd.DataFrame({\n",
    "        \"missing_before\": before,\n",
    "        \"missing_after\": after,\n",
    "        \"filled\": (before - after).clip(lower=0)\n",
    "    })\n",
    "    report.to_csv(output_dir / \"missing_value_report.csv\")\n",
    "    return df_imputed\n",
    "\n",
    "# ----------------------------------------\n",
    "# WEEK 10: Syntactic & Pattern-Based Cleaning\n",
    "def syntactic_cleaning(df):\n",
    "    print(\"\\n=== WEEK 10: SYNTACTIC CLEANING ===\")\n",
    "    if \"product_category\" in df.columns:\n",
    "        df[\"product_category\"] = (\n",
    "            df[\"product_category\"]\n",
    "              .str.lower().str.strip()\n",
    "              .str.replace(r\"\\s+\", \" \", regex=True)\n",
    "        )\n",
    "        print(\" • Cleaned product_category whitespace & casing\")\n",
    "    else:\n",
    "        print(\" • No product_category column to clean\")\n",
    "    return df\n",
    "\n",
    "# ----------------------------------------\n",
    "def main():\n",
    "    # reproducibility\n",
    "    np.random.seed(0)\n",
    "    random.seed(0)\n",
    "\n",
    "    base_dir = Path.home() / \"Downloads\" / \"IS 537\"\n",
    "    sales_files   = sorted(base_dir.glob(\"*category-sales*.csv\"))\n",
    "    events_files  = sorted(base_dir.glob(\"champaign-events-*.md\"))\n",
    "    weather_files = sorted(base_dir.glob(\"champaign_il_weather_*.csv\"))\n",
    "\n",
    "    # load mapping\n",
    "    cat_map = load_category_mapping(base_dir / \"categories.json\")\n",
    "\n",
    "    # ─── load & aggregate sales\n",
    "    sales_list = [\n",
    "        aggregate_sales_by_category(read_sales_data(fp), cat_map)\n",
    "        for fp in sales_files\n",
    "    ]\n",
    "    sales_df = pd.concat(sales_list).groupby(level=0).mean()\n",
    "\n",
    "    # ─── load & pivot events\n",
    "    events_list = [parse_events(fp) for fp in events_files]\n",
    "    events_df   = pd.concat(events_list).groupby(level=0).max()\n",
    "\n",
    "    # ─── load weather\n",
    "    weather_df = None\n",
    "    if weather_files:\n",
    "        wdfs = []\n",
    "        for fp in weather_files:\n",
    "            dfw = pd.read_csv(fp)\n",
    "            dfw[\"date\"] = pd.to_datetime(dfw[\"date\"])\n",
    "            dfw.set_index(\"date\", inplace=True)\n",
    "            wdfs.append(dfw.interpolate())\n",
    "        weather_df = pd.concat(wdfs).groupby(level=0).mean()\n",
    "\n",
    "    # ─── carve out date range, clipping future dates\n",
    "    min_date   = min(sales_df.index.min(), events_df.index.min())\n",
    "    today      = pd.Timestamp.today().normalize()\n",
    "    max_source = max(sales_df.index.max(), events_df.index.max())\n",
    "    max_date   = min(today, max_source)\n",
    "    all_dates  = pd.date_range(start=min_date, end=max_date)\n",
    "\n",
    "    # ─── build combined\n",
    "    combined = pd.DataFrame(index=all_dates)\n",
    "    combined = combined.join(sales_df).join(events_df).fillna(0)\n",
    "    if weather_df is not None:\n",
    "        combined = combined.join(weather_df)\n",
    "\n",
    "    # reset index for profiling\n",
    "    df0 = combined.reset_index().rename(columns={\"index\":\"date\"})\n",
    "\n",
    "    # ───────────────────────────────────────────────────────────────\n",
    "    # Week 5: Profiling\n",
    "    profile_data(df0, base_dir / \"profiling_outputs\")\n",
    "\n",
    "    # ───────────────────────────────────────────────────────────────\n",
    "    # Week 6: Missing-Value Handling\n",
    "    df1 = df0.set_index(\"date\")\n",
    "    df1 = handle_missing_values(df1, base_dir / \"imputation_reports\")\n",
    "    df1 = df1.reset_index()\n",
    "\n",
    "    # ───────────────────────────────────────────────────────────────\n",
    "    # Week 7: Anomaly Detection (flag only)\n",
    "    Q1  = df1[\"Total Sales\"].quantile(0.25)\n",
    "    Q3  = df1[\"Total Sales\"].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    df1[\"is_outlier_iqr\"] = ((df1[\"Total Sales\"] < Q1 - 1.5*IQR) |\n",
    "                             (df1[\"Total Sales\"] > Q3 + 1.5*IQR))\n",
    "    df1[\"zscore\"]       = stats.zscore(df1[\"Total Sales\"].fillna(0))\n",
    "    df1[\"is_outlier_z\"] = df1[\"zscore\"].abs() > 3\n",
    "    print(f\"[Week 7] Flagged {df1['is_outlier_iqr'].sum()} IQR outliers and \"\n",
    "          f\"{df1['is_outlier_z'].sum()} Z-score outliers\")\n",
    "\n",
    "    # ───────────────────────────────────────────────────────────────\n",
    "    # Week 8: Duplicate Detection & Handling\n",
    "    df1[\"is_duplicate_date\"] = df1.duplicated(subset=[\"date\"], keep=False)\n",
    "    n_dup = df1[\"is_duplicate_date\"].sum()\n",
    "    print(f\"[Week 8] Found {n_dup} rows in duplicate-date groups\")\n",
    "    pre = len(df1)\n",
    "    df1 = df1.drop_duplicates(subset=[\"date\"], keep=\"first\")\n",
    "    post = len(df1)\n",
    "    print(f\"[Week 8] Dropped {pre-post} duplicates; {post} rows remain\")\n",
    "\n",
    "    # ───────────────────────────────────────────────────────────────\n",
    "    # Week 10: Syntactic Cleaning\n",
    "    df1 = syntactic_cleaning(df1)\n",
    "\n",
    "    # ───────────────────────────────────────────────────────────────\n",
    "    # Week 11: Constraint Checks\n",
    "    print(\"\\n=== WEEK 11: CONSTRAINT VALIDATION ===\")\n",
    "    negs = df1[df1[\"Total Sales\"] < 0]\n",
    "    print(f\" • Found {len(negs)} negative Total Sales entries\")\n",
    "    df1 = df1[df1[\"Total Sales\"] >= 0]\n",
    "    # sum-of-categories vs. total\n",
    "    cat_cols = [c for c in df1.columns if c not in [\"date\",\"Total Sales\",\"zscore\",\n",
    "                                                     \"is_outlier_iqr\",\"is_outlier_z\",\n",
    "                                                     \"is_duplicate_date\"]]\n",
    "    mismatches = (df1[cat_cols].sum(axis=1).round(2) != df1[\"Total Sales\"].round(2)).sum()\n",
    "    print(f\" • Found {mismatches} rows where category sums ≠ Total Sales\")\n",
    "\n",
    "    # Add holiday flag with pandas US calendar\n",
    "    cal = USFederalHolidayCalendar()\n",
    "    holidays = cal.holidays(start=min_date, end=max_date)\n",
    "    df1[\"is_holiday\"] = df1[\"date\"].isin(holidays)\n",
    "    print(f\" • Marked {df1['is_holiday'].sum()} federal holidays\")\n",
    "\n",
    "    # ───────────────────────────────────────────────────────────────\n",
    "    # Week 12: Probabilistic Cleaning Signals\n",
    "    print(\"\\n=== WEEK 12: PROBABILISTIC SIGNALS ===\")\n",
    "    # combine signals into one error indicator\n",
    "    df1[\"has_error_signal\"] = (\n",
    "        df1[\"is_outlier_iqr\"] |\n",
    "        df1[\"is_outlier_z\"] |\n",
    "        df1[\"is_duplicate_date\"] |\n",
    "        df1[\"is_holiday\"]  # if you want to treat holiday days differently\n",
    "    ).astype(int)\n",
    "    print(f\" • Computed probabilistic error signal for {df1['has_error_signal'].sum()} rows\")\n",
    "\n",
    "    # ───────────────────────────────────────────────────────────────\n",
    "    # Week 13: Integration Sanity Checks\n",
    "    print(\"\\n=== WEEK 13: INTEGRATION CHECKS ===\")\n",
    "    # check for any dates missing weather or events columns\n",
    "    missing_weather = df1[\"temperature\"].isna().sum() if \"temperature\" in df1.columns else 0\n",
    "    print(f\" • Missing weather on {missing_weather} days\")\n",
    "    # ... add any other domain‐specific integration checks here\n",
    "\n",
    "    # ───────────────────────────────────────────────────────────────\n",
    "    # Week 14: Workflow Provenance & Environment Snapshot\n",
    "\n",
    "# Week 14: Workflow Provenance & Environment Snapshot\n",
    "    print(\"\\n=== WEEK 14: PROVENANCE & ENVIRONMENT ===\")\n",
    "    env = {\n",
    "        \"python\":   platform.python_version(),\n",
    "        \"pandas\":   pd.__version__,\n",
    "        \"numpy\":    np.__version__,\n",
    "        \"scipy\":    scipy.__version__,\n",
    "        \"timestamp\": datetime.utcnow().isoformat()\n",
    "    }\n",
    "    with open(base_dir / \"env_snapshot.json\", \"w\") as f:\n",
    "        json.dump(env, f, indent=2)\n",
    "    print(f\"✓ Wrote environment snapshot to {base_dir/'env_snapshot.json'}\")\n",
    "\n",
    "    # ───────────────────────────────────────────────────────────────\n",
    "    # Week 15: Reproducibility & Transparency\n",
    "    print(\"\\n=== WEEK 15: REPRODUCIBILITY ===\")\n",
    "    print(\" • Random seeds set; code fully versioned in GitHub repo;\")\n",
    "    print(\"   env snapshot & profiling artifacts saved for full traceability\")\n",
    "\n",
    "    # ───────────────────────────────────────────────────────────────\n",
    "    # Write out final\n",
    "    out_path = base_dir / \"combined_data_full.csv\"\n",
    "    df1.to_csv(out_path, index=False)\n",
    "    print(f\"\\n✅ All done — combined data → {out_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "616a8ef481e1e2077a9027a351cf30569c6d04c9007284c32600bfe4f84d2717"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
